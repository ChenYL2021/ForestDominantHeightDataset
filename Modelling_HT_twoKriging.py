# -*- coding: utf-8 -*-
# @Time    : 2025/10/1 21:42
# @Author  : ChenYuling
# @Software: PyCharm
# @Describeï¼šå¯¹æ¯”ä¸¤ç§æ–¹å¼è¿›è¡Œä¼˜åŠ¿é«˜æ¨¡å‹æ„å»º

#%%å¿½ç•¥ä¸€äº›ç‰ˆæœ¬ä¸å…¼å®¹ç­‰è­¦å‘Š
import warnings
warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt
#%read data
import seaborn as sns
import numpy as np
import pandas as pd

from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import ExtraTreeRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
import xgboost as xgb
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
# from deepforest import CascadeForestRegressor
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
import optuna
from optuna.visualization import plot_optimization_history
from optuna.visualization import plot_param_importances
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score
from math import sqrt
import pandas as pd
import numpy as np
import lightgbm as lgb
import gstools as gs
import lightgbm as lgb
import optuna
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

#%%#####################################################################################################################
###########################################è¯»å–æ•°æ®è¿›è¡Œå“‘å˜é‡åŒ–å¤„ç†#################################################
########################################################################################################################
#%read data
# df1 = pd.read_csv('./DATA/TrainDATA30m.csv', sep=',')
df1 = pd.read_csv(r'K:\WorkingNotes\TH\WN\20251005\DATA\data2.csv',encoding = 'gb2312') #,encoding = 'gb2312'
#%
sym90 = pd.read_csv('./DATA/sym90.csv', sep=',')#trmcä»£ç è½¬åç§°, encoding = 'gb2312'
data1 = pd.merge(df1, sym90, on='trmc', how='left')
#%
data2 = data1[['X', 'Y','HT',
        'bio_1', 'bio_2', 'bio_3', 'bio_4', 'bio_5', 'bio_6', 'bio_7', 'bio_8', 'bio_9', 'bio_10',
        'bio_11', 'bio_12', 'bio_13', 'bio_14', 'bio_15', 'bio_16', 'bio_17', 'bio_18', 'bio_19', 'age',
        'VH', 'VV', 'tchd', 'trzd', 'aspect', 'elevation', 'slope', 'pnf', 'NDVI_MAX', 'SU_SYM90_y','REGION']]
data2 = data2.dropna(axis=0,how='any')
data2_filtered = data2[data2['pnf'] != 0]
#%å¤„ç†ç¦»æ•£æ•°æ®-å“‘å˜é‡å¤„ç†
# å°†åˆ—è½¬æ¢ä¸ºå­—ç¬¦ç±»å‹
# data2['trzd'] = data2['trzd'].round().astype('Int64')
data2_filtered['trzd'] = data2_filtered['trzd'].astype(str)
# data2['pnf'] = data2['pnf'].round().astype('Int64')
data2_filtered['pnf'] = data2_filtered['pnf'].astype(str)
# sub_df['pnf'] = round(sub_df['pnf'])
data3 = pd.get_dummies(
    data2_filtered,
    columns=['trzd','SU_SYM90_y','pnf'],
    prefix=['TRZD','TRMC','PNF'],
    prefix_sep="_",
    dummy_na=False,
    drop_first=False)
data3.columns
#%%#####################################################################################################################
"""
å…¨å›½æ£®æ—æ ·åœ°å»ºæ¨¡è„šæœ¬ï¼ˆHTä¸ºç›®æ ‡å˜é‡ï¼‰
æµç¨‹ï¼š
1. Declusteringï¼ˆ1km Grid-basedï¼‰
2. LightGBMå»ºæ¨¡
3. æ®‹å·®Coarse-grid Kriging
4. è¾“å‡ºé¢„æµ‹+ä¸ç¡®å®šåº¦
"""


#% =========================
# 1. Declustering: ç”Ÿæ€åŒº + ç©ºé—´ç½‘æ ¼æƒé‡
# =========================
def compute_ecogrid_weights(df, x_col='X', y_col='Y', eco_col='REGION', grid_size=1000, all_ecos=None):
    weights = pd.Series(0.0, index=df.index)
    if all_ecos is None:
        all_ecos = df[eco_col].unique()

    for eco in all_ecos:
        eco_df = df[df[eco_col] == eco].copy()
        if eco_df.empty:
            continue

        # ç©ºé—´ç½‘æ ¼ç´¢å¼•
        eco_df['grid_x'] = (eco_df[x_col] // grid_size).astype(int)
        eco_df['grid_y'] = (eco_df[y_col] // grid_size).astype(int)
        eco_df['grid_id'] = eco_df['grid_x'].astype(str) + "_" + eco_df['grid_y'].astype(str)

        # ç½‘æ ¼å†…æƒé‡ = 1 / ç½‘æ ¼ç‚¹æ•°
        grid_counts = eco_df.groupby('grid_id').size()
        eco_df['weight'] = eco_df['grid_id'].map(lambda g: 1.0 / grid_counts[g])

        # å½’ä¸€åŒ–ï¼Œä½¿è¯¥ç”Ÿæ€åŒºæ€»æƒé‡ = 1
        eco_df['weight'] = eco_df['weight'] / eco_df['weight'].sum()
        weights.loc[eco_df.index] = eco_df['weight']

    return weights


# è®¡ç®—æƒé‡
all_eco_ids = data3['REGION'].unique()
weights = compute_ecogrid_weights(data3, grid_size=1000, all_ecos=all_eco_ids)
data3['weight'] = weights

#%% ============ 3. LightGBMå»ºæ¨¡ ============åŠŸèƒ½: ä½¿ç”¨ Declustering æƒé‡ + è‡ªåŠ¨è°ƒå‚ + æ—©åœ + å¯è§†åŒ–
# ==========================================================
# 3.1. æ•°æ®å‡†å¤‡
# ==========================================================
# å› å˜é‡ä¸è‡ªå˜é‡
y = data3['HT'].astype(float).values
X = data3.drop(columns=['HT', 'X', 'Y','REGION']).copy()
w = data3['weight'].astype(float).values

# é˜²æ­¢å†…å­˜ä¸è¶³ï¼šLightGBM èƒ½åŸç”Ÿæ”¯æŒ float32
X = X.astype(np.float32)

# åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†
X_train, X_valid, y_train, y_valid, w_train, w_valid = train_test_split(
    X, y, w, test_size=1/3, random_state=2025
)

train_data = lgb.Dataset(X_train, label=y_train, weight=w_train)
valid_data = lgb.Dataset(X_valid, label=y_valid, weight=w_valid)

#%% ==========================================================
# 3.2 Optuna + LightGBM è°ƒå‚
# ==========================================================
def objective(trial):
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 31, 120),
        'max_depth': trial.suggest_int('max_depth', 5, 16),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.95),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 1000),
        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
        'seed': 2025,
        'feature_pre_filter': False  # å¯ç•™ä¹Ÿå¯åˆ ï¼Œå·²æ— å†²çª
    }

    # âš ï¸ æ¯æ¬¡ trial å†…éƒ¨é‡å»º Datasetï¼Œé˜²æ­¢å‚æ•°å†²çª
    train_data_trial = lgb.Dataset(X_train, label=y_train)
    valid_data_trial = lgb.Dataset(X_valid, label=y_valid)

    # è®­ç»ƒæ¨¡å‹
    gbm = lgb.train(
        params,
        train_data_trial,
        num_boost_round=3000,
        valid_sets=[valid_data_trial],
        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
    )

    # é¢„æµ‹ä¸è¯„ä»·
    preds = gbm.predict(X_valid, num_iteration=gbm.best_iteration)
    rmse = np.sqrt(mean_squared_error(y_valid, preds))
    return rmse

# =======================
# 3ï¸.3  è¿è¡Œ Optuna ä¼˜åŒ–
# =======================
print("\nâ³ å¼€å§‹ Optuna è‡ªåŠ¨è°ƒå‚ ...")
study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=2025))
study.optimize(objective, n_trials=100, show_progress_bar=True)


#%% ==========================================================
# 3.4. è¾“å‡ºæœ€ä¼˜ç»“æœ
# ==========================================================
print("\nâœ… æœ€ä¼˜ RMSE:", study.best_value)
print("âœ… æœ€ä¼˜å‚æ•°ï¼š")
for k, v in study.best_params.items():
    print(f"  {k}: {v}")

#%% ==========================================================
# 3.5. ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
# ==========================================================
best_params = study.best_params.copy()
best_params.update({
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'verbosity': -1,
})

# å°è¯•GPUåŠ é€Ÿ
try:
    best_params["device_type"] = "gpu"
    print("âš™ï¸ ä½¿ç”¨ GPU åŠ é€Ÿè®­ç»ƒ")
except Exception:
    pass

train_data = lgb.Dataset(X, label=y, weight=w)

print(f"ğŸ” å½“å‰ LightGBM ç‰ˆæœ¬: {lgb.__version__}")
print("ğŸ“˜ ä½¿ç”¨ callback å®ç°æ—©åœ (å…¨ç‰ˆæœ¬å…¼å®¹)")

# âœ… ä½¿ç”¨ callback æ§åˆ¶æ—©åœä¸æ—¥å¿—
callbacks = [
    lgb.early_stopping(stopping_rounds=100),
    lgb.log_evaluation(period=200)
]

final_model = lgb.train(
    params=best_params,
    train_set=train_data,
    num_boost_round=2000,
    valid_sets=[train_data],
    valid_names=["train"],
    callbacks=callbacks
)

# final_model.save_model("LGBM_Optuna_ChinaForestModel.txt")
# print("\nğŸ“¦ æ¨¡å‹å·²ä¿å­˜ï¼šLGBM_Optuna_ChinaForestModel.txt")
#%% ==========================================================
# 3.6. ä¿å­˜æ¨¡å‹ä¸ Optuna ç»“æœ
# ==========================================================
import optuna
import lightgbm as lgb
import matplotlib.pyplot as plt
import joblib
from datetime import datetime
import os
# è‡ªåŠ¨ä¿å­˜è·¯å¾„
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_path = f"LGBM_Optuna_ChinaForestModel_{timestamp}.txt"
study_csv = f"Optuna_Study_{timestamp}.csv"
study_pkl = f"Optuna_Study_{timestamp}.pkl"

# ç¡®ä¿å½“å‰ç›®å½•ä¸‹ä¿å­˜
final_model.save_model(model_path)
joblib.dump(study, study_pkl)

# Optuna trials æ•°æ®ä¿å­˜ä¸º CSV
df_trials = study.trials_dataframe()
df_trials.to_csv(study_csv, index=False)

print(f"\nğŸ“¦ æ¨¡å‹å·²ä¿å­˜: {model_path}")
print(f"ğŸ“Š Optunaç»“æœå·²ä¿å­˜ä¸ºï¼š\n- CSV: {study_csv}\n- PKL: {study_pkl}")

# âœ… éªŒè¯ä¿å­˜æ•ˆæœ
print(f"\næœ€ä¼˜RMSE: {study.best_value:.4f}")
print("æœ€ä¼˜å‚æ•°ï¼š")
for k, v in study.best_params.items():
    print(f"  {k}: {v}")

#%% ==========================================================
# 7. å¯è§†åŒ–ç»“æœ
# ==========================================================
import os
from datetime import datetime
import numpy as np
import pandas as pd
import plotly.io as pio
import optuna
import gstools as gs
try:
    import optuna.visualization as vis

    # RMSE ä¼˜åŒ–å†å²
    fig1 = vis.plot_optimization_history(study)
    fig1.update_layout(title="Optuna RMSE Optimization History")

    # å‚æ•°é‡è¦æ€§
    fig2 = vis.plot_param_importances(study)
    fig2.update_layout(title="Optuna Parameter Importance")

    # æ˜¾ç¤ºäº¤äº’å¼å›¾
    if show_browser:
        fig1.show(renderer="browser")
        fig2.show(renderer="browser")

    # ä¿å­˜é™æ€å›¾
    try:
        rmse_path = os.path.join(vis_dir, f"RMSE_history_{timestamp}.png")
        param_path = os.path.join(vis_dir, f"Param_importance_{timestamp}.png")
        pio.write_image(fig1, rmse_path, scale=3)
        pio.write_image(fig2, param_path, scale=3)
        print(f"âœ… å¯è§†åŒ–å›¾å·²ä¿å­˜ï¼š\n- {rmse_path}\n- {param_path}")
    except Exception:
        print("âš ï¸ PNG å¯¼å‡ºå¤±è´¥ï¼Œè¯·ç¡®è®¤ kaleido å·²å®‰è£…ï¼špip install -U kaleido")
except Exception as e:
    print("âš ï¸ Optuna å¯è§†åŒ–ä¸å¯ç”¨ï¼Œè¯·å®‰è£…ï¼špip install optuna[visualization] plotly")
    print("è¯¦ç»†é”™è¯¯ï¼š", e)

#%% ============ 4. Coarse-grid Krigingï¼ˆæ®‹å·®ï¼‰ ============
print("Running coarse-grid kriging on residuals...")

# èšåˆæ®‹å·®åˆ° coarse grid (1 km)
coarse_size = 1000
data3['cx'] = (data3['X'] // coarse_size) * coarse_size + coarse_size / 2
data3['cy'] = (data3['Y'] // coarse_size) * coarse_size + coarse_size / 2

coarse_df = data3.groupby(['cx', 'cy']).agg({'residual':'mean'}).reset_index()
cx, cy, cz = coarse_df['cx'].values, coarse_df['cy'].values, coarse_df['residual'].values

# æ‹Ÿåˆå˜å·®å‡½æ•°
model_vario = gs.Exponential(dim=2)
fit_vario = gs.vario_estimate_unstructured((cx, cy), cz, bin_num=20)
model_vario.fit_variogram((fit_vario[0], fit_vario[1]), nugget=True)

# æ„å»ºkrigingå¯¹è±¡
ok = gs.krige.Ordinary(model_vario, cond_pos=(cx, cy), cond_val=cz)

# åœ¨æ ·ç‚¹ä½ç½®é¢„æµ‹æ®‹å·®ä¿®æ­£é‡
pred_r, var_r = ok((data3['X'].values, data3['Y'].values))
data3['kriged_residual'] = pred_r
data3['kriged_var'] = var_r

# ============ 5. åˆæˆæœ€ç»ˆé¢„æµ‹ä¸ä¸ç¡®å®šåº¦ ============
data3['HT_final'] = data3['pred_lgbm'] + data3['kriged_residual']
data3['HT_sd'] = np.sqrt(data3['kriged_var'])

# ============ 6. ä¿å­˜ç»“æœ ============
out_cols = ['X', 'Y', 'HT', 'HT_final', 'HT_sd', 'pred_lgbm', 'kriged_residual']
data3[out_cols].to_csv("HT_model_results.csv", index=False)
print("âœ… å®Œæˆå»ºæ¨¡ï¼ç»“æœå·²ä¿å­˜ä¸º HT_model_results.csv")

# ============ 7. å¯é€‰ï¼šè¾“å‡ºæ¨¡å‹ç‰¹å¾é‡è¦æ€§ ============
import matplotlib.pyplot as plt
lgb.plot_importance(model, max_num_features=20, figsize=(8,6))
plt.tight_layout()
plt.savefig("feature_importance.jpg", dpi=600)
plt.close()
print("Feature importance plot saved.")

#%%#####################################################################################################################
###########################################ç‹¬ç«‹æ£€éªŒæ•°æ®é›†æ±‡æ€»ç»“æœï¼ˆæœªè°ƒå‚æ•°ï¼‰#################################################
########################################################################################################################
#%% ==========================================================
# å…¨å›½æ£®æ—æ ·åœ°å»ºæ¨¡æµç¨‹å°è£…å‡½æ•°
#=============================================================
#%% ==========================================================
# å…¨å›½æ£®æ—æ ·åœ°å»ºæ¨¡æµç¨‹ï¼ˆè¿ç»­è„šæœ¬ç‰ˆï¼‰
# 1ï¸âƒ£ Declustering + 2ï¸âƒ£ LightGBM + Optuna è°ƒå‚ + 3ï¸âƒ£ Kriging
#=============================================================
import os
import numpy as np
import pandas as pd
import lightgbm as lgb
import optuna
import gstools as gs
import plotly.io as pio
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from datetime import datetime
import joblib
#%%read data
import pandas as pd

# 1è¯»å–æ•°æ®
# df1 = pd.read_csv('./DATA/TrainDATA30m.csv', sep=',')
df1 = pd.read_csv(r'K:\WorkingNotes\TH\WN\20251005\DATA\data2.csv', encoding='gb2312')

sym90 = pd.read_csv('./DATA/sym90.csv', sep=',')  # trmcä»£ç è½¬åç§°

#  åˆå¹¶æ•°æ®
data1 = pd.merge(df1, sym90, on='trmc', how='left')

# ç­›é€‰æ‰€éœ€åˆ— & åˆ é™¤ç¼ºå¤±
cols = ['X', 'Y','HT','REGION',
        'bio_1', 'bio_2', 'bio_3', 'bio_4', 'bio_5', 'bio_6', 'bio_7', 'bio_8', 'bio_9', 'bio_10',
        'bio_11', 'bio_12', 'bio_13', 'bio_14', 'bio_15', 'bio_16', 'bio_17', 'bio_18', 'bio_19', 'age',
        'VH', 'VV', 'tchd', 'trzd', 'aspect', 'elevation', 'slope', 'pnf', 'NDVI_MAX', 'SU_SYM90_y']
data2 = data1[cols].dropna(axis=0, how='any')

# è¿‡æ»¤ pnf éé›¶
data2_filtered = data2.loc[data2['pnf'] != 0].copy()

# å¤„ç†ç¦»æ•£åˆ—-å“‘å˜é‡
data2_filtered.loc[:, 'trzd'] = data2_filtered['trzd'].astype(str)
data2_filtered.loc[:, 'pnf'] = data2_filtered['pnf'].astype(str)

# å“‘å˜é‡å¤„ç†
data3 = pd.get_dummies(
    data2_filtered,
    columns=['trzd','SU_SYM90_y','pnf'],
    prefix=['TRZD','TRMC','PNF'],
    prefix_sep="_",
    dummy_na=False,
    drop_first=False
)

# æŸ¥çœ‹åˆ—å
print(data3.columns)

#%% ===============================
# é…ç½®å‚æ•°
# ===============================
target = 'HT'
x_col, y_col = 'X', 'Y'
eco_col = 'REGION'
grid_size = 1000
coarse_size = 0.01 #0.01 åº¦ï¼ˆçº¦ 1 kmï¼‰
test_size = 1/3
n_trials = 50
vis_dir = "Optuna_Figures"
save_dir = "ChinaForestModel"
show_browser = True
seed = 2025

os.makedirs(vis_dir, exist_ok=True)
os.makedirs(save_dir, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

#%% ===============================
# 1ï¸âƒ£ Declustering æƒé‡è®¡ç®—
# ===============================
def compute_ecogrid_weights(df, x_col, y_col, eco_col, grid_size):
    weights = pd.Series(0.0, index=df.index)
    all_ecos = df[eco_col].unique()

    for eco in all_ecos:
        eco_df = df[df[eco_col] == eco].copy()
        if eco_df.empty:
            continue

        eco_df['grid_x'] = (eco_df[x_col] // grid_size).astype(int)
        eco_df['grid_y'] = (eco_df[y_col] // grid_size).astype(int)
        eco_df['grid_id'] = eco_df['grid_x'].astype(str) + "_" + eco_df['grid_y'].astype(str)

        grid_counts = eco_df.groupby('grid_id').size()
        eco_df['weight'] = eco_df['grid_id'].map(lambda g: 1.0 / grid_counts[g])
        eco_df['weight'] = eco_df['weight'] / eco_df['weight'].sum()
        weights.loc[eco_df.index] = eco_df['weight']

    return weights

data3['weight'] = compute_ecogrid_weights(data3, x_col, y_col, eco_col, grid_size)

#%% ===============================
# 2ï¸âƒ£ LightGBM å»ºæ¨¡ + Optuna è°ƒå‚
# ===============================
y = data3[target].astype(float).values
X = data3.drop(columns=[target, x_col, y_col, eco_col]).copy()
w = data3['weight'].astype(float).values
X = X.astype(np.float32)

# åˆ’åˆ†è®­ç»ƒé›†
X_train, X_valid, y_train, y_valid, w_train, w_valid = train_test_split(
    X, y, w, test_size=test_size, random_state=seed
)

# Optuna ç›®æ ‡å‡½æ•°
def objective(trial):
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 31, 120),
        'max_depth': trial.suggest_int('max_depth', 5, 16),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.95),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 1000),
        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
        'seed': seed,
    }

    train_data_trial = lgb.Dataset(X_train, label=y_train)
    valid_data_trial = lgb.Dataset(X_valid, label=y_valid)

    gbm = lgb.train(
        params,
        train_data_trial,
        num_boost_round=3000,
        valid_sets=[valid_data_trial],
        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]
    )

    preds = gbm.predict(X_valid, num_iteration=gbm.best_iteration)
    rmse = np.sqrt(mean_squared_error(y_valid, preds))
    return rmse

# Optuna è°ƒå‚
print("\nâ³ å¼€å§‹ Optuna è‡ªåŠ¨è°ƒå‚ ...")
study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=seed))
study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
print(f"\nâœ… æœ€ä¼˜ RMSE: {study.best_value:.4f}")

#%% ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
best_params = study.best_params.copy()
best_params.update({
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'verbosity': -1,
})
try:
    best_params['device_type'] = 'gpu'
    print("âš™ï¸ ä½¿ç”¨ GPU åŠ é€Ÿè®­ç»ƒ")
except:
    print("CPUè¿è¡Œï¼")
    pass

train_data_full = lgb.Dataset(X, label=y, weight=w)
callbacks = [lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=200)]
final_model = lgb.train(
    params=best_params,
    train_set=train_data_full,
    num_boost_round=2000,
    valid_sets=[train_data_full],
    valid_names=["train"],
    callbacks=callbacks
)

# ä¿å­˜æ¨¡å‹å’Œ Optuna study
model_path = os.path.join(save_dir, f"LGBM_Optuna_ChinaForestModel_{timestamp}.txt")
final_model.save_model(model_path)
study_pkl = os.path.join(save_dir, f"Optuna_Study_{timestamp}.pkl")
joblib.dump(study, study_pkl)
print(f"ğŸ“¦ æ¨¡å‹å·²ä¿å­˜: {model_path}")
print(f"ğŸ“Š Optuna study å·²ä¿å­˜: {study_pkl}")

#%% ===============================
# 3ï¸âƒ£ å¯è§†åŒ– Optuna ç»“æœ
# ===============================
try:
    import optuna.visualization as vis
    fig1 = vis.plot_optimization_history(study)
    fig1.update_layout(title="Optuna RMSE Optimization History")
    fig2 = vis.plot_param_importances(study)
    fig2.update_layout(title="Optuna Parameter Importance")

    if show_browser:
        fig1.show(renderer="browser")
        fig2.show(renderer="browser")

    rmse_path = os.path.join(vis_dir, f"RMSE_history_{timestamp}.png")
    param_path = os.path.join(vis_dir, f"Param_importance_{timestamp}.png")
    try:
        pio.write_image(fig1, rmse_path, scale=3)
        pio.write_image(fig2, param_path, scale=3)
        print(f"âœ… å¯è§†åŒ–å›¾å·²ä¿å­˜ï¼š\n- {rmse_path}\n- {param_path}")
    except:
        print("âš ï¸ PNG å¯¼å‡ºå¤±è´¥ï¼Œè¯·å®‰è£… kaleido: pip install -U kaleido")
except Exception as e:
    print("âš ï¸ Optuna å¯è§†åŒ–ä¸å¯ç”¨ï¼Œè¯·å®‰è£… optuna[visualization] plotly")
    print(e)

#%% ===============================
# 4ï¸âƒ£ è®¡ç®—æ®‹å·® & Coarse-grid Kriging
# ===============================
preds_full = final_model.predict(X)
data3['residual'] = y - preds_full

# è®¾ç½® coarse grid
data3['cx'] = (data3[x_col] // coarse_size) * coarse_size + coarse_size / 2
data3['cy'] = (data3[y_col] // coarse_size) * coarse_size + coarse_size / 2

# èšåˆè®¡ç®— coarse æ®‹å·®
coarse_df = data3.groupby(['cx', 'cy']).agg({'residual': 'mean'}).reset_index()
cx, cy, cz = coarse_df['cx'].values, coarse_df['cy'].values, coarse_df['residual'].values

# è®¡ç®—ç»éªŒå˜å¼‚å‡½æ•°
# bin_center, gamma = gs.vario_estimate_unstructured((cx, cy), cz)
# è®¾ç½®æœ€å¤§è·ç¦»ä¸ºæ ·ç‚¹é—´æœ€å¤§è·ç¦»çš„ä¸€åŠï¼Œåˆ†10~15ä¸ªbinè¾ƒåˆé€‚
max_dist = np.max(np.sqrt((cx[:, None] - cx[None, :])**2 + (cy[:, None] - cy[None, :])**2)) / 2
bin_edges = np.linspace(0, max_dist, 16)  # 15ä¸ªbin -> 16ä¸ªè¾¹ç•Œ
bin_center, gamma = gs.vario_estimate_unstructured(
    (cx, cy), cz,
    bin_edges=bin_edges
)

# å»é™¤nanå€¼
mask = ~np.isnan(gamma)
bin_center, gamma = bin_center[mask], gamma[mask]
#å®šä¹‰ç†è®ºå˜å·®æ¨¡å‹
model_vario = gs.Exponential(dim=2)
#æ‹Ÿåˆå˜å·®å‡½æ•°
model_vario.fit_variogram(bin_center, gamma, nugget=True)
#æ™®é€šå…‹é‡Œé‡‘æ’å€¼
ok = gs.krige.Ordinary(model_vario, cond_pos=(cx, cy), cond_val=cz)
pred_r, var_r = ok((data3[x_col].values, data3[y_col].values))

# ä¿å­˜ç»“æœ
data3['kriged_residual'] = pred_r
data3['kriged_var'] = var_r
data3['pred_final'] = preds_full + pred_r

print("âœ… å…¨å›½æ£®æ—æ ·åœ°å»ºæ¨¡å®Œæˆï¼Œç»“æœåŒ…å« ['weight','residual','kriged_residual','kriged_var','pred_final']")
#%%
import matplotlib.pyplot as plt
plt.figure(figsize=(6,4))
plt.scatter(bin_center, gamma, label="Empirical Variogram", color="black", s=35)
plt.plot(bin_center, model_vario.variogram(bin_center),
         label="Fitted Exponential Model", color="red", lw=2)
plt.xlabel("Lag distance (m)", fontsize=11)
plt.ylabel("Semivariance Î³(h)", fontsize=11)
plt.title("Residual Variogram Fitting", fontsize=12)
plt.legend(frameon=False)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()
#%%
data3.to_csv(r"./ChinaForestModel/data3.csv", index=False, encoding="utf-8-sig")